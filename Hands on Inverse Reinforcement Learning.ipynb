{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the True Rewards\n",
    "\n",
    "#TODO\n",
    "true_rewards = np.zeros(16)\n",
    "true_rewards[15] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_from_v(env, V, s, gamma=1):\n",
    "    \n",
    "    q = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q[a] += prob * (reward + gamma * V[next_state])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=1):\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    for s in range(env.nS):\n",
    "        q = q_from_v(env, V, s, gamma)\n",
    "        \n",
    "        # OPTION 1: construct a deterministic policy \n",
    "        # policy[s][np.argmax(q)] = 1\n",
    "        \n",
    "        # OPTION 2: construct a stochastic policy that puts equal probability on maximizing actions\n",
    "        best_a = np.argwhere(q==np.max(q)).flatten()\n",
    "        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n",
    "        \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            V[s] = max(q_from_v(env, V, s, gamma))\n",
    "            delta = max(delta,abs(V[s]-v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\n",
      "[[1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_vi, V_vi = value_iteration(env.env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_vi,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRewardWrapper(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env, rewards):\n",
    "        super(MyRewardWrapper, self).__init__(env)\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, true_reward, terminated, info = self.env.step(action)\n",
    "        reward = self.rewards[next_state]\n",
    "        return next_state, reward, terminated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = np.random.randn(16)\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "# env = MyRewardWrapper(env, rewards = r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Expert Trajectories Using Optimal Policy\n",
    "num_trajs = 1000\n",
    "expert_trajs = []\n",
    "for i in range(num_trajs):\n",
    "    traj = []\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    is_done = False\n",
    "    while True:\n",
    "        action = np.argmax(policy_vi[state])\n",
    "        traj.append((state, action))\n",
    "        next_state, reward, is_done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if is_done:\n",
    "            break\n",
    "        \n",
    "    expert_trajs.append(traj)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_env = gym.make('FrozenLake-v0')\n",
    "# original_env.env.P\n",
    "\n",
    "\n",
    "# adding +1 to account for absorbing state\n",
    "        # (reached whenever game ended)\n",
    "n_states = original_env.observation_space.n + 1\n",
    "n_actions = original_env.action_space.n\n",
    "\n",
    "transitions = np.zeros([n_states, n_actions, n_states])\n",
    "\n",
    "# iterate over all \"from\" states:\n",
    "for state, transitions_given_state in original_env.env.P.items():\n",
    "    # iterate over all actions:\n",
    "    for action, outcomes in transitions_given_state.items():\n",
    "        # iterate over all possible outcomes:\n",
    "        for probability, next_state, _, done in outcomes:\n",
    "            # add transition probability T(s, a, s')\n",
    "            transitions[state, action, next_state] += probability\n",
    "            if done:\n",
    "                # outcome was marked as ending the game.\n",
    "                # if game is done and state == next_state, map to absorbing state instead\n",
    "                if state == next_state:\n",
    "                    transitions[state, action, next_state] = 0\n",
    "                # map next state to absorbing state\n",
    "                # make sure that next state wasn't mapped to any other state yet\n",
    "                assert np.sum(transitions[next_state, :, :-1]) == 0\n",
    "                transitions[next_state, :, -1] = 1.0\n",
    "\n",
    "# specify transition probabilities for absorbing state:\n",
    "        # returning to itself for all actions.\n",
    "transitions[-1, :, -1] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svf(trajs, transition_matrix, policy):\n",
    "    \n",
    "#     longest_traj_len = np.max([len(traj) for traj in trajs()])\n",
    "    longest_traj_len = 100\n",
    "    nS = 16 # TODO: remove hardcoding\n",
    "    nA = 4 \n",
    "            \n",
    "        \n",
    "    # svf[state, time] is the frequency of visiting a state at some point of time\n",
    "    svf = np.zeros((nS, longest_traj_len))\n",
    "\n",
    "    for traj in trajs:\n",
    "        for state in traj[:,0]:\n",
    "#             print(state)\n",
    "            svf[int(state), 0] += 1\n",
    "    svf[:, 0] = svf[:, 0] / len(trajs)\n",
    "\n",
    "    for time in range(1, longest_traj_len):\n",
    "        for state in range(nS):\n",
    "            total = 0\n",
    "            for previous_state in range(nS):\n",
    "                for action in range(nA):\n",
    "                    total += svf[\n",
    "                        previous_state, time - 1] * transition_matrix[\n",
    "                            previous_state, action, state] * policy[\n",
    "                                previous_state, action]\n",
    "            svf[state, time] = total\n",
    "    # sum over all time steps and return SVF for each state:\n",
    "    return np.sum(svf, axis=1)   \n",
    "\n",
    "# compute_svf(trajs, transitions, policy = np.random.randn(16, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape expert trajectories for convenience\n",
    "\n",
    "trajs = np.zeros((1000, 100, 2))\n",
    "\n",
    "for i, traj in enumerate(expert_trajs):\n",
    "    tmp = np.zeros((100, 2))\n",
    "    tmp[:np.array(traj).shape[0]] = np.array(traj)\n",
    "    trajs[i] = tmp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49370283  0.83372422 -0.47692964 -0.31729862  1.15796439  0.60233496\n",
      " -0.60407374  1.7635248   0.43640086  0.60269814  0.31637755 -2.35735379\n",
      "  1.16237767  1.71425276  0.43251223  0.30506572]\n",
      "[ 0.00000000e+000  2.96311838e-007 -1.19610884e-007 -1.82892919e-005\n",
      "  0.00000000e+000  6.02334959e-001 -1.83298045e-020  1.76352480e+000\n",
      "  0.00000000e+000  3.10617073e-253  9.94352710e-060 -2.35735379e+000\n",
      "  1.16237767e+000  1.81500025e-065  2.42321984e-079  2.24850692e+187]\n"
     ]
    }
   ],
   "source": [
    "# ME_IRL Begins\n",
    "\n",
    "## initialize reward fucntion\n",
    "\n",
    "# reward is a Fully Connected NN with two layers\n",
    "\n",
    "# reward_model = nn.Sequential(nn.Linear(100, 32),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(32, 16))\n",
    "\n",
    "\n",
    "num_iters = 100\n",
    "\n",
    "theta = np.random.randn(16)\n",
    "print(theta)\n",
    "# rewards = torch.matmul(theta, torch.Tensor(np.eye(16)))\n",
    "\n",
    "def rf(state):\n",
    "    return theta[int(state)]\n",
    "\n",
    "def rt(traj):\n",
    "    return np.sum([rf(state) for state in traj[:, 0]])\n",
    "\n",
    "def grad(trajs, theta, svf):\n",
    "    \n",
    "    first_term = []\n",
    "    for traj in trajs:\n",
    "        drtaudpsi = np.zeros(16)\n",
    "        for i, state in enumerate(traj[:, 0]):\n",
    "            drtaudpsi[int(state)] = rf(state)\n",
    "        first_term.append(drtaudpsi)\n",
    "    first_term = np.sum(first_term, axis = 0)\n",
    "    \n",
    "#     svf = compute_svf(trajs, transitions, policy)\n",
    "    for state in range(16):\n",
    "        second_term = np.zeros(16)\n",
    "        \n",
    "        thing = svf[state]*rf(state)\n",
    "        second_term[state] = thing\n",
    "    \n",
    "    gr = first_term - 1000*second_term\n",
    "    \n",
    "    return gr\n",
    "\n",
    "\n",
    "# the training loop\n",
    "\n",
    "lr = 0.001\n",
    "        \n",
    "    \n",
    "for i in range(num_iters):\n",
    "    # wrap env with current reward estimate\n",
    "    env_now = MyRewardWrapper(env, rewards)\n",
    "    # solve using value iteration\n",
    "    policy, _ = value_iteration(env_now.env.env)\n",
    "    \n",
    "    # get state-visitation-frquency\n",
    "    \n",
    "    svf = compute_svf(trajs,transitions, policy)\n",
    "    \n",
    "    gr = grad(trajs, theta, svf)\n",
    "    \n",
    "   \n",
    "    theta -= gr*lr\n",
    "#     print(theta)\n",
    "    \n",
    "    \n",
    "print(theta)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randn(16)\n",
    "r = torch.Tensor(r)\n",
    "reward(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reward_model(torch.Tensor(trajs[2][:,0]))\n",
    "a = torch.sum(r)\n",
    "a.backward()\n",
    "\n",
    "\n",
    "grads = []\n",
    "params = reward_model.parameters()\n",
    "for i, param in enumerate(params):\n",
    "    grads.append(param.grad)\n",
    "    \n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    param -= grads[i]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3010, -0.0735,  0.0607,  0.2353, -0.0014,  0.3910,  0.0201,  0.0372,\n",
       "         0.0367, -0.0868, -0.0566,  0.2312, -0.1701,  0.0402, -0.1171,  0.0302],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_from_traj(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-dc4678547bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "reward.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.core import RewardWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RewardWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RewardWrapper<TimeLimit<FrozenLakeEnv<FrozenLake-v0>>>>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trenton/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'gym.core.RewardWrapper'> doesn't implement 'reward' method. Maybe it implements deprecated '_reward' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RewardWrapper' object has no attribute '_reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-40ed2d6195cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreward\u001b[0;34m(self, reward)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mdeprecated_warn_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s doesn't implement 'reward' method. Maybe it implements deprecated '_reward' method.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RewardWrapper' object has no attribute '_reward'"
     ]
    }
   ],
   "source": [
    "env.reward(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
